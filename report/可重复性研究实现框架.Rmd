---
title: "可重复性研究流程的实现-以医学临床评价为例"
output: word_document
---

##一、可重复性研究的现状


可重复性是传统科学研究中的重要准则[^何华清]，在传统的科学观里，可重复性是被认为自明和不可或缺的。它是科学研究中准确性，确定性的基础，无法重复的研究也就无法保证结果的真实性和结论的稳健性。可重复性研究的想法来源于上世纪80年代和90年代早期，由Jon Claerbout和他领导的Stanford Exploration Project。当时可重复性研究被当做一种辅助教学和研究的标准来使用，Jon发现新学生想要了之前的研究成果需要花费大量的时间和精力，制定可重复性研究标准可以保障学术的连贯性，避免对同一个问题的重复分析。这一想法后来被Jonathan B. Buckheit and David L. Donoho加以深入的研究，建立了Wavelab library[^Wavelab]，形成了一套以matlab为平台的综合体系。可重复性研究开始被广泛重视的标志性事件是美国杜克大学波蒂学术问题事件[^波蒂学术事件]。2006年，波蒂教授领导的团队，在《自然-医学》杂志上发表有关肺癌的研究成果，引起同行的广泛关注。一年后，杜克大学根据此研究方法，启动了三个临床实验项目。为了谨慎起见，美国德克萨斯大学安德森癌症中心的临床研究者邀请了Keith Baggerly 和Kevin Coombes两位生物统计学家领导独立的团队对论文进行核实，但始终无法重现论文结果。在花费了近2000个小时的深入探查后，他们发现了杜克团队论文中出现的一些低级错误，包括不属于数据清单的基因、数据错位及混乱的标签分组等，但该研究结果并未引起杜克方面的重视。2010年7月，波蒂被证实简历造假，随后发起的独立审查结果证实论文中存在数据处理和分析错误。2010年底，杜克大学永久停止了那三项临床实验。之后，近20篇与此论文相关并在顶级医学期刊上发表的论文相继退稿。杜克事件的影响远远超出事件本身，这一事件使人们开始重视可重复性方面的研究[^1]并取得了一定的成果[^Rob]。

[^Wavelab]: David Donoho,Arian Maleki(2008)."15 Years of Reproducible Research in Computational Harmonic Analysis"

[^Rob]: Rob J Hyndman(2009). "Encouraging replication and reproducible research"

[^1]: M. Barni, F. Perez-Gonzalez, P. Comesaña, and G. Bartoli, Putting reproducible signal processing into practice: A case study in watermarking, in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, vol. 4, April 2007, pp. 1261–1264. 

##二、为什么需要可重复性研究

###2.1 科学研究的性质要求
在统计科学，计算科学，数据科学领域，涉及到大量的数据和算法概念，仅仅用文本的形式（论文或者书籍）无法将这些概念准确的传达给读者。‘单纯的论文并不能被看做是学术成就，而仅仅是在给自己的成果打广告’。科学研究成果不应仅仅告诉读者‘做了什么’，还应改包含‘怎么做’，这其中包含着，数据处理的每个流程。这一方面是让读者真正的了解这一理论的过程，同时也是验证其理论正确性的有效途径。解决这一问题的一个有效方法是提供论文相关的计算代码和数据。比如，论文的作者会将自己的数据和代码放到网站上以供别人下载使用。然而在实际操作过程中，读者往往面临着代码不完整、代码可读性太差、运行环境不统一、系统和软件版本有差异等诸多的问题。

在2012年，一项由“Begley”和“Ellis”发起的统计研究显示，过去十年内，在《Natrue》上发表的有关癌症研究的53篇论文当中，有47篇是不可重复的[^可重复性研究的比例]。这些研究中不可重复的原因集中为：无法重复试验、无法控制实验结果，无法重现所有的数据，统计测试方法的错误使用等等[^无法可重复研究的原因]。论文、成果的检验需要耗费大量的时间成本，在波蒂学术事件中，研究团队重复波蒂团队的研究成果花费了近2000个小时。如此高昂的成本使得对论文的重复验证工作变得极为困难。同时，随着大数据时代的来临，统计相关的科学领域中涉及到越来越多的数据，越来越复杂的模型。这进一步增加了论文成果验证的难度。

随着大数据时代的来临，数据变得越来越庞大，数据结构也更加复杂，数据的维数极大的增加。处理这些数据所使用的技术和方案是非常类似的。虽然计算能力也在随着数据的增长而增长，但重复的计算大量的数据仍然需要大量的时间成本和预算成本。


[^可重复性研究的比例]: Begley, C. G.; Ellis, L. M. (2012). "Drug development: Raise standards for preclinical cancer research". Nature 483 (7391): 531–533. doi:10.1038/483531a. PMID 22460880. edit
[^无法可重复研究的原因]: Begley, CG (2013). "Reproducibility: six flags for suspect work". Nature 497: 433–434. doi:10.1038/497433a


###2.2 其他方面的问题

2.2.1、项目的连续性和合作方面的问题

一项研究需要持续很长的时间，并且需要大量人员的参与，这意味着科学有效的管理和小组成员间的成果共享变得非常的重要。例如：一项癌症临床试验往往持续数年时间，其中的涉及到多次的数据统计和分析，而且需要大量的人员合作完成。但是，不规范的项目管理以及文档的不可重复极大地增加了合作人员之间沟通的难度，即便是某一位研究人员，想要重复数月以前的数据分析过程也非常的困难。研究人员经常面临着诸如：“这些代码以前见过。这些数据文件是从何而来的”，‘我发现原始数据有问题’，'这段脚本不能运行了'等问题，而解决这些问题所花费的成本并不亚于第一次处理数据的成本。

2.2.2、知识的累积的困难

人类的文明，是通过知识积累来形成的。知识记载和传承的方式决定了知识积累的速度。21世纪信息化时代的来临，知识也进入了爆炸式增长的时代，但是，知识记载和传承的方式并没有改进，大量的成果仍然仅仅是以纯文本的方式发布。虽然，读者可以通过互联网获取这些文本，但想要真正理解这些研究理论，掌握这些知识，单纯的依靠论文是远远不够的。读者在阅读论文时会遇到各种问题，这些问题与论文写作过程中作者的问题可能很类似，但作者并没有将这些问题的解决方案记录下来。最糟糕的是，如果读者想要重复论文中的结果，那么他将不得不花费和作者同样的时间去处理诸如‘数据清洗’等方面的问题。这极大的降低了知识积累和创新的效率。


2.2.3、数据科学任务中的时间分配

在数据科学领域中，数据清洗在开发时间和预算方面占到了30%~80%的比例。但是，数据的不规范性并不是研究本身造成的，而是由于数据收录的不规范性，研究的核心应该是模型和结果。这种本末倒置的现象在数据科学的各个领域非常的普遍。而且，对于同一个研究过程，不同的人员需要花费同样的时间和预算成本去处理数据。数据清洗也是科学论文检验过程中花费成本最多的地方。

2.2.4、科研效率问题

可重复性首先是对研究者本身而言的，从短期来看，实现上述的标准需要投入大量额外的时间，但从长远上看，实现可重复性可以大大的提高效率，从而节约时间。就单一的项目而言，项目的所花费的时间和项目的复杂度成正比，如果项目的周期很长，那么前期数据处理、资料整理就变得异常的重要，在项目进行过程中可能出现各种意外情况，在进行到项目中期或后期，数据源改变、中间步骤数据出错等哪怕是很小的问题都可能需要投入大量的时间去修正，因为一旦数据改变，所有相关的图表和描述都需要重新生成，如果你足够有耐心，就需要把之前的过程重新来过，而且祈祷数据上不会再出现问题。另外，几个月前的你对于现在来说就如同陌生人，你不可能记住六个月之前做过的与项目相关的具体工作，那么重新调试函数和数据将会成为噩梦。从另一方面，一个人的研究领域是很固定的，这就意味着一个人在相关领域的能力或只是储备是依靠不断积累来增长的，这就类似于盖房子，可重复性研究可以保证地基的稳定性，也就保证了研究的效率。



###2.3 当前推广可重复性研究的困难

可重复性研究至今没有得到足够的推广和重视，原因可以归结为两个方面[^3]。
一方面，可重复性研究缺乏统一的标准。至今为止，可重复性研究并没有统一的标准，它仅仅作为科学研究的一个准则，但并没有约束性。这使得研究人员将更多的精力放到研究的结论上面，而不去关心文章的可重复性。
另一方面，实现可重复性研究的成本太高，实现可重复性，意味着作者需要一方面关注研究的过程和结论，另一方面关注研究的可重复性，比如将代码写的足够的规范，作者需要投入大量的时间将数据和结果以可重复性的结果（比如在web上）展现出来。这会大大的分散作者的注意力。同时，实现可重复性需要大量的辅助工具，学习这些工具会极大的增加学习成本。本文介绍的可重复研究实现的方案，可以在有效的解决这两方面的问题。

[^3]: J. Kovacevic, How to encourage and publish reproducible research, in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, vol. 4, April 2007, pp. 1273–1276. 



##三、可重复性研究的定义

###3.1 可重复性研究的定义

可重复性研究是指基于同一原始研究资料，既可以由作者自己的分析工具生成该论文展示的研究成果，也可以由他人在别的分析平台中用相似的分析过程重现一样的结果[^4]。可重复性研究并没有确切的定义，它是一个独立而完备的概念。可重复性是指：独立的研究人员在获取相关的数据和代码后可以重复全部的论文结果，包括数据的获取、数据清洗的过程，图表等元素。例如，在医学研究的基因组学领域，除了需要作者提交芯片等原始数据，还要求提供分析流程及所用的程序代码。对于作者而言，遵循可重复性原则的作者可以在不同的时间，在不同的机器上通过重新运行项目来修正原始的报告。而对于读者而言，由于作者已经提供了重复作者工作的左右数据细节，读者可以轻松的修改程序，然后得到修改后的结果（图、表、结论等）。
在波蒂学术事件中，研究人员为了重新在线杜克大学波蒂团队所发表的论文的结果，虽然课可以获取波蒂团队提交的数据，但由于缺乏具体的分析过程，他们的核查工作耗费了进2000个小时，可见为了学术自身净化与及时纠错，数据共享和程序提交很有必要，为了原始研究结果出来后高效率地流程化分析及确保此后的重现性，值得花费大量的时间与精力用于程序代码的编写和调整。例如，在目前医学等领域处理海量数据时，结合R语言和Knitr包生成动态报表。

[^4]: P. Marziliano, Reproducible research: A case study of sampling signals with finite rate of innovation, in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, vol. 4, April 2007, pp. 1265–1268.

可重复性是数据分析和其他计算科学的基础标准之一，可重复性的研究结果并不意味是正确的，比如代码中可能包含bug，或者模型中存在缺陷，甚至人为的伪造实验。但可重复性研究可以保证数据处理流程的规范性，从而避免数据处理过程中错误的发生概率。

一般来说，单项研究本身的样本量越大，结论越有说服力，在与其他同理研究比较时证据强度越大；在数据和分析流程固定的情况下，一项研究自身的结果被很好的重现，那么他人就你能借鉴研究结论的可接受程度；自身重现性好的研究更有可能被同类研究所重复，从而确认研究结果的真实性和结论的稳健性。

###3.2 文学化编程和自动化报告
实现可重复性研究的关键在于记录数据处理和分析的整个过程，文学化编程[^Electronic]是解决这一问题的重要技术。
文学化编程也成为动态编程，是用自然语言写出来的对于程序逻辑的解释，程序中包含宏和传统的代码段。在文学化编程中，宏与标题类似，是用人类语言描述的解释性短语，与计算机科学中的伪代码类似，将代码段或者更低层次的宏隐藏起来。文学化编程是由高德纳[^高德纳]于1981年提出并在Tex排版系统中引入的，使用pascal作为基础的编程语言，使用Tex作为文档的排版工具。

[^高德纳]: D. E. Knuth, Literate Programming, The Computer Journal, vol. 27, no. 2, pp. 97–111, May 1984.

[^Electronic]: J. Claerbout, Electronic documents give reproducible research a new meaning, in Proc. 62nd Ann. Int. Meeting of the Soc. of Exploration Geophysics, 1992, pp. 601–604. 

文学化编程需要支持两种类型的转换：编译和排版。编译是将文字内容和代码块以一种适合机器处理的方式组织到一起。排版的的目的是使文档能够阅读。在文学化报告中，代码和文本都是以“chunk”的形式存在，‘块’与’‘块’之间保持独立性，同时又能相互调用，代码块的作用是处理数据和得出计算结果，文本块则是解释论文的内容。满足了可重复性中模块化和复用性的要求。代码块可以运行相应的代码，比如R，python，甚至java。代码块会根据语言标记的不同调用不同的解释器运行代码。文本块则与普通的文本相同，但它有一个重要的特性就是内链其他语言的代码。此外，文学化编程还需要另外两种工具，一种是语言的解释器，这取决于代码块或内联函数中使用的语言类型；另一个工具是转化器，代码块是供机器阅读的，最终形成的报告中不需要包括这些代码，而只需要文本块部分，因而，转化器的作用就是将代码块和文本块转化为最终的报告形式（比如pdf或者Html）。

2001年，Temple Lang使用R和XML引入了一种动态处理和交互式文学化编辑的技术。2002年，Leisch发明了Sweave[^sweave]，将R语言环境和LaTex技术结合到一起。

[^sweave]: F. Leisch, Sweave: Dynamic generation of statistical reports using literate data analysis, in Compstat 2002 — Proceedings in Computational Statistics, W. Härdle and B. Rönz, Eds. Physica Verlag, Heidelberg, 2002, pp. 575–580, ISBN 3-7908-1517-9. 

文学化编程和自动化报告不仅仅是将代码和文本结合到一起，同时这些元素也可以被独立的抽取运行，这对于作者和读者而言都是非常方便的。
###3.3 可重复性和可复制性

在给出可重复性研究的定义之前，有必要区分可重复性研究与其他概念，比如’可复制性‘、’独立性验证‘，’科学实验意义上的可重复‘，可重复性要求在获取数据和代码之后可以重复出相同的数据结果，与操作人员无关，这要求数据和代码中应包含重复论文所需要的全部材料。可复制性或者独立性验证则要求第三方研究人员在在相似的实验环境下，重复整个实验步骤后得出相同的结果，比如物理化学实验。可重复性研究保证的是从数据到结论的规范性、一致性。但不能保证实验方法是够符合科学原则。

##四、实现可重复性研究的基本原则

本文将讨论实现可重复性研究的一般原则和主要工具，这仅仅只是实现可重复性的一种方案[^2],遵循这些基本原则和使用这些通用的工具可以在数据科学的任何领域中实现。

[^2]: S. Fomel and G. Hennenfent, Reproducible computational experiments using scons, in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, vol. 4, pp. 1257–1

###4.1 所有的步骤通过代码实现

重现论文中的所有元素[^eco]，比如图表和结果，首先要保证的是数据的一致性。因而，每一步对于数据的处理，都应以代码的形式保留数据处理的每一个细节。比如，很多的研究成果中的数据是从网络上获取的，多数情况下，作者会将数据以excel或者其他的形式直接从网上下载到本地，但有的数据，网站并未提供规范化后的格式以供下载，而是一XML等标记语言记录在网页表格当中，如果此时采用手动抄录的方式将数据记录下来，那么数据的准确性就得不到保证。在可重复性研究中，数据的每一步操作都需要通过代码完成，即便是获取数据的过程。
重现论文的另一个重要方面是保证图表的一致性。在保证数据一致性的基础下，图表的一致性与对图表的操作直接相关，这就要求，在生成图表的过程中，必须避免使用鼠标、键盘通过粘贴、复制的方式修改图表，与数据一致性的要求相同，所有对于图表的操作都应该通过代码来完成。
第三个方面，重现论文过程中必须保持结果的一致性。在数据处理过程中，伴随着中间结果的产生，为了保证每一步结果的可重复性，要求所有的数据结果都应该保存在变量中，而对于结果的调用，则通过代码来实现。在计算过程中出现的数据不能被粘贴到文本或者脚本当中。

[^eco]: R. Koenker, Reproducible Econometric Research, Department of Econometrics, University of Illinois, Urbana-Champaign, IL, Tech. Rep., 1996. 

###4.2 所有的过程自动实现

手动的操作会使论文的可重复性大大降低，比如项目中的某一个处理过程是通过粘贴和复制实现的，那么对于独立研究人员而言，很难在重复出这一过程。这与前一条原则类似，这里强调的是文件操作和数据流的自动实现。在整个项目过程中，文件、文件夹的生成，数据的导入、导出、合并、删减等过程，都应该通过代码实现。这一技术要求可以将通过Make和Command Line这两种脚本语言来实现。



###4.3 提供的数据尽可能保持原始的形式
项目从一开始便伴随着数据处理，而数据处理就意味着有出错的风险，只有最原始的数据保留着最真实、最完整的信息。重复性构建论文或对论文进行验证的过程也应该从最原始的数据开始。需要特别指出的是，实际工作中往往涉及到保护用户个人隐私，保护知识产权等问题，数据不能公开，此时可以通过加密的方式来实现数据共享。另一方面，很难确定原始数据的范畴，任何数据在获取之前都已经经过了一定的预处理，比如问卷调查中的数据应该是最原始的数据，但这样的数据不可能用来做数据分析，而是经过统计人员统计之后才能处理。因而，这里的原始数据是指在写项目开始之前尚未处理的数据。
同时，在对数据进行处理时，应该在原始数据的基础上生成的新的数据，并保存在新的数据集中，而不能修改原始的数据。原始数据和过程中处理的数据应该分开保存。


###4.4 注明所有数据文件的来源

保持数据一致性的另一个要求就是注明数据文件的来源，比如，从互联网上获取的数据，随着时间变化，数据的地址和内容都可能发生变化，因而，为了保证结果的一致性，所有的数据文件都应包在说明文档中注明来源，其中应包含时间、地址和校验信息。

###4.5 保证数据和代码的完备性

从原始数据到最终用于分析的数据，以及每一步分析过程需要的数据，任何一个步骤上出现问题都会导致意外的结果。前面的原则可以保证数据处理都通过代码来实现，代码的完备性要求，任何涉及到输入输出的代码都应该记录在脚本文件中，而不应单独在控制台中运行（控制台中的代码无法保存），每一个过程中的数据集都可以通过运行脚本中的某一个代码块来实现。这一要求进一步保证了数据前后的一致性。


###4.6 提供详细的说明文档

详细的说明文档包含以下两个方面：
1、代码注释：代码注释是计算机编程的基本要求之一，它本身就是可重复性研究的一部分。良好的代码注释不仅帮助读者理解代码的含义，也可以帮助作者有效的管理大量的代码。
2、运行时环境的说明：受到计算机系统、软件版本、硬件配置方面的差异影响，在不同的运行时环境下，同样的运算过程可能出现不同的结果。比如，在python2.x版本和python3.x版本之间，存在大量的差异。这并不是研究人员可以修正的问题，这就需要在最终的说明文档中，应该包含软件版本这一重要的影响要素。除此之外，说明文档中应包含环境配置的帮助，以本文的可重复性研究为例，在readme.md文件中，包含着如何安装package的步骤，以及运行实例和可交互式程序的操作步骤。

###4.7 版本控制

版本控制是项目管理中的重要一环，一项成果再发布之后也需要不断的更新代码和结果。版本控制可以避免由于错误的修改而导致不可重复。同时也提供了项目演进的过程，给可重复性研究提供标准。


##五、实现可重复性研究的工具

这一部分将介绍实现可重复性研究的工具和作用。这些工具最终将被整合到统一的开发环境下。

###5.1 使用 Make 自动运行脚本和函数，控制文件流和数据流

make是软件开发中的自动化构建软件。它是一种转化文件形式的工具，转换的目标称为“target”；与此同时，它也检查文件的依赖关系，如果需要的话，它会调用一些外部软件来完成任务。使用Make可以自动生成并记录工作流程，记录数据文件和代码之间的相关性。Make的一个重要特性是根据需要只运行修改后的代码，这可以极大的提高重复的效率。这一工具保证了‘所有过程自动实现’的原则
    
###5.2 使用命令行操作文档、控制工作流

在可重复性的原则下，应该尽量避免使用鼠标以及粘贴、复制等手动操作。命令行可以实现文件和文件夹的创建、移动、复制、修改等。同时，命令行可以记录数据流和文件流。

###5.3 RMarkdown编辑样式
  
markdown是一种轻量级的标记语言，可以很方便的翻译为HTML语言。markdown的出现简化了HTML，把常用的HTML标签用极度简化的语法写出来。在传统的论文写作中，研究人员需要浪费大量的时间去思考排版，而markdown将排版从写作中独立出来，研究人员只需要关注写作，而样式的调整通过设计CSS样式来实现。一般而言，可以直接套用CSS的样式。Rmarkdown则在markdown的基础上整合了R语言，将数据和代码放到了统一的文件中，Rmarkdown是文学化编程的核心。
	
###5.4 使用 Knitr 动态生成报告，进行文学化编程

knitr是由Sweave改进而来，由Yihui Xie开发[]，它的作用是文学化编程和自动化报告中的转化器，它能够识别并编译自动化报告中的R代码块。这些R代码块使用一些特定的标记，通过这些标记还可以设定代码运行的一些选项。Knitr在设计之初考虑了网页的格式，它有两种实现结果：一种是原始的HTML格式，即把R代码嵌入HTML代码；二是Markdown，结合起来形成了Rmrkdown。
	
###5.5 使用 Git 控制版本
  
Git是一款开源的分布式版本控制系统，它可以有效、高速的处理从很小到很大的项目版本管理。使用版本控制可以方便的查询历史修改记录，提供多人同时测试，也可以避免由于意外情况（如停电）或错误修改导致的程序破坏。
  
###5.6 使用R packages[^package]拓展应用

本文的可重复性实现方案是以R语言为平台建立的。R语言是一款开源软件。一方面，R语言目前是进行数据科学计算领域的主流语言之一。另一方面，R语言目前已经有数千的包，极大地拓展了R语言的能力，在本文的可重复性研究方案中，主要涉及到以下几个包：
	
* reshape2 ：主要对数据集进行数据糅合；
* dplyr：高效的进行数据清洗
* ggplot2：引入图层的概念，简化画图的流程
* shiny：做交互花程序
* Rcurl ：用于网络数据爬取
* D3js：制作可交互式图表

在本文的实例中，最后的所有可重复性研究的材料将以R包的形式封装，这是R的另一个特性，用户可以轻松的开发自己的R包。这样，读者只需要从Github上远程安装R包，就可以实现对论文的可重复性研究。
	
    
[^package]: McMurdie PJ, Holmes S (2013) phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLoS ONE 8(4): e61217. doi:10.1371/journal.pone.0061217    
    
###5.7 使用Rstdio整合

为了实现文学化编程和自动化报告中的所有功能，需要支持R，支持Knitr，支持markdown的编辑器。Rstdio是一款开源软件，它集成了Rmarkdown、Sweave、R等关键技术，同时可以调用shell命令行程序，同时，它还集成了git的语法，可以方面的进行版本控制。


###5.8 对其他语言的兼容 

本文中的可重复性研究方案使用的是以R语言为基础的平台，然而，R语言并不适合处理所有的任务。比如网页数据爬取的，R语言的效率是非常低的，这就意味着，需要使用其他语言来弥补R语言在这些方面的缺陷。幸运的是，在R中，有相应的package提供了面向其他语言的接口函数。比如：rjava包提供了面向java语言的接口。rpy2则提供了R语言下python的接口。在本文的实例中，将介绍在R平台下，调用python进行网页数据爬取的任务。

	
###5.9 大数据的支持和缓存技术
Rmarkdown引入了缓存的技术方案，解决大规模的数据处理和计算问题。在Rmarkdown中，没有经过任何修改（即便是一个空格）代码块，在下次运行时会被直接跳过。这就有效的降低了文档中运行R代码的负荷。对Knitr而言，考虑过于严格会导致不会要的重复计算，所以，它基本只检查代码和选项是否变化，而这些二外的要求可以有用户进行定制。一个缓存的代码段会在第一次运行的时候吧新创建的对象都写入缓存文件，在下次运行时从文件中直接加载他们。这就好像代码重新被运行过一样。
大数据是未来数据科学处理的主要对象。在R平台下，两种主要的大数据处理技术都用相应的R解决方案。Hadoop中友HadoopR接口，Spark中提供了SparkR接口。但这两类技术不在本文的讨论范围内。
	

##六、实现可重复性研究的步骤

本节将介绍在传统的研究流程基础上改进的可重复性的研究流程。
传统研究过程一般包括，定义问题，收集数据，数据清洗，数据探索，建立模型，模型结果的解释，模型验证等步骤。但这一步骤并不能保证可重复性的实现。本文修改了传统的研究步骤，使其符合可重复性研究的原则。具体步骤如下：

1、确定研究问题：研究问题的确定直接影响到数据的获取，但这不是可重复性研究的重点。

2、搭建工作环境：首先根据研究的问题获取原始的数据，获取数据的过程应包含在单独的脚本中。之后，在Rstdio中建立‘项目’，调用shell建立文件管理系统，生成项目目录。在R语言内设置工作路径。文件系统可以根据自身的需要设计，但需要遵循以下几个原则：

* 将原始数据和处理后的数据分开存储；
* 将分析代码、文档、数据分离；
* 建立test文件夹进行测试
* 建立README.md说明文档
* 使用makefile文件确定文件流
* 使用相对路径代替绝对路径
	
3、建立版本信息：使用Git创建第一个版本，并上传到云平台中备份

4、数据清洗[^数据清洗] ：数据清洗过程借助R语言reshape2包完成，形成独立的脚本并与处理后的数据保存在一起

5、在Rmarkdown中完成数据探索，建立模型，模型解释和模型检验，生成可重复性报告：如前文所述，Rmarkdown整合了文档和代码，是实现可重复性研究的核心。对于处理单项任务的函数，可以放置到单独的脚本中。用户可以一边书写文档，一边检测代码运行的结果，生成图表等可重复性元素。

[^数据清洗]: 叶鸥,张璟,李军怀. 中文数据清洗研究综述[J]. 计算机工程与应用,2012,14:121-129.

##七、可重复性研究实现的实例

在本节中，将以分析UCI的机器学习数据集中的帕金森数据为例，实现上文中的可重复性研究方案。

###7.1 研究问题描述

文本说是用的数据集由牛津大学“Max Littl”实验室与NCVS合作收集而来，并被UCI收录到机器学习标准数据集中。

该数据集包含了对31位测试对象的一系列生物医学声音检测数据，在31位测试对象中，有23位患有帕金森症。每一位测试对象共有6次测试记录。本文的研究问题是建立合适的模型将健康人群与患有帕金森症的病人区分开.

###7.2 搭建工作环境

本文已经实现可重复性，并将版本上传至github主页中。读者可以从github上clone该项目的可重复性文件，或者从github上安装package，即可在本地进行测试。无需重复搭建环境。
需要注意的是，本文是在mac环境下搭建的，因而在windowns平台下运行时需要修改工作路径。跨平台兼容问题是可重复性研究的一个方面，但本文并不包含该方面的解决方案。
该项目的工作流程和工作目录结构见图7.1,图7.2：

图7.1：项目流程图
![1](/Users/xiao_lisp/Desktop/project/report/jpg2.jpg)

图7.2：工作目录
![2](/Users/xiao_lisp/Desktop/project/report/jpg1.jpg)

### 7.3 数据清洗

数据清洗的过程包含在data文件夹下的yuchuli.R文件中

### 7.4 使用Rmakrdown进行文学化编程生成动态报告

在进行动态报告之前，需要在Rstdio中配置合适的开发环境，主要包含该项目运行时的版本信息，以及安装程序需要的包.执行该过程的代码块如下所示：

```{r,message=FALSE}
sessionInfo() #显示版本信息

#导入需要的包，如果读者仅下载了动态化报告（Rmd文件），则需要自行安装以下的包，如果读者从github上安装了本项目的包，则以下package会自自行安装
library(knitr)
library(caret) 
library(kernlab)
library(pROC)

##设置工作目录,clone此项目时使用读者自定义的工作目录即可
##数据导入
setwd('/Users/xiao_lisp/Desktop/project')
sick <- read.csv('rawdata/parkinsons.data')


###数据清洗，只需要导入数据清洗的脚本即可
source('data/yuchuli.R')
```

对数据进行简单的统计描述，各个变量的箱线图统计如图：
```{r,echo=FALSE,fig.height= 3,fig.width=12,results='asis'}
par(cex.axis = 1,mfcol = c(1,3))
sick_new$status <- as.factor(sick_new$status)
for(i in 2:23)
    plot(sick_new$status,sick_new[,i],xlab=names(sick_new[,i]),col ='red',main = names(sick_new[,i]))
```

患有帕金森症的患者和正常人之间在22种测试指标上，其均值都有明显的差异。

本文采用svm作为分类模型对数据进行分类，首先随机的选择75%的数据作为训练数据，余下25%的数据作为测试数据，选取过程如下：
```{r}
#设置随机参数
set.seed(100)
sam <- sample(1:195,size = 146)
#训练数据
sick_new_train <- sick_new[sam,]

#测试数据
sick_new_test <- sick_new[-sam,]
```
在R中，set.seed函数可以保证重复此次随机过程时产生相同的序列，从而保证可重复性。
在选取高斯函数作为核函数进行训练和预测之后，预测结果如表所示：
```{r,echo=FALSE,message=FALSE,warning=FALSE,results='asis'}
sick_new$status <- as.integer(as.character(sick_new$status))
predictor1 <- ksvm(status~.,data = sick_new_train,kernel = 'rbfdot',default = "automatic")
sick_predictor <- predict(predictor1,sick_new_test)
tab1 <- table(sick_predictor,sick_new_test$status)
tab1k <- matrix(c(4,8,1,36),nrow =2,ncol= 2,byrow = F, 
                dimnames = list(c("健康", "帕金森"),c("健康", "帕金森")))
kable(tab1k,format = "markdown", align = "c")
```

在不进行特征选择的情况下，正确率为`r round((tab1[1]+tab1[4])/sum(tab1),3)*100`%,假阳性率仅为`r round(tab1[3]/sum(tab1),3)*100`%，分类的效果十分的明显。

### 7.5 特征选择与模型优化

下表显示的是22个参数之间的相关程度，可以看出，在22个生理学指标中，部分指标的相关程度很高，为了进一步提高模型的准确度，需要进行特征筛选。

```{r,echo=FALSE,results='asis'}
correlationMatrix <- cor(sick_new[,c(2:23)])
kable(correlationMatrix[1:8,1:8])
```

使用ROC曲线分析，对22个参数的重要程度排名，结果如图所示：
```{r,echo=FALSE}
control <- trainControl(method = 'repeatedcv',number =10,repeats = 3)
model <- train(status~.,data = sick_new,method = 'svmRadial', preProcess="scale", trControl=control)
importance <- varImp(model, scale=FALSE)
#print(importance)
par(family='GB1')
plot(importance,main = 'pic7.4:importance')
```

为了选择出最佳的特征子集，使用随机森林对特征的所有组合进行测试，设置迭代次数为20次，结果如图7.5所示：
```{r,echo=FALSE,cache=TRUE}
par(family='STKaiti')
svmProfile <- rfe(sick_new[,2:23],sick_new$status,sizes = c(2:23),rfeControl = rfeControl(functions = caretFuncs,method = 'cv',number = 5),method = 'svmRadial')
#print(svmProfile)
#predictors(svmProfile)
plot(svmProfile,type = c('g','o'),main = 'pic7.5')
```

当特征选择为`r predictors(svmProfile)`组成的子集时，RMSE达到最低值.此时的预测情况为：
```{r,echo=FALSE,message=FALSE,warning=FALSE,results='asis'}
#sick_new$status <- as.integer(as.character(sick_new$status))
sick_new_train_new <- sick_new_train[,c('status',predictors(svmProfile))]
sick_new_test_new <- sick_new_test[,c('status',predictors(svmProfile))]
predictor2 <- ksvm(status~.,data = sick_new_train_new,kernel = 'rbfdot',default = "automatic")
sick_predictor <- predict(predictor2,sick_new_test_new)
tab2 <- table(sick_predictor,sick_new_test_new$status)
tab1k <- matrix(c(8,5,0,36),nrow =2,ncol= 2,byrow = F, 
                dimnames = list(c("健康", "帕金森"),c("健康", "帕金森")))
kable(tab1k,format = "markdown", align = "c")
```

经过特征选择，正确率为`r round(44/49,3)*100`%，达到了非常理想的效果。


##八、可重复性研究的意义

**对于研究人员自身**：可重复性首先是对研究者本身而言的，从短期来看，实现上述的标准需要投入大量额外的时间，但从长远上看，实现可重复性可以大大的提高效率，从而节约时间。就单一的项目而言，项目的所花费的时间和项目的复杂度成正比，如果项目的周期很长，那么前期数据处理、资料整理就变得异常的重要，在项目进行过程中可能出现各种意外情况，在进行到项目中期或后期，数据源改变、中间步骤数据出错等哪怕是很小的问题都可能需要投入大量的时间去修正，因为一旦数据改变，所有相关的图表和描述都需要重新生成，如果你足够有耐心，就需要把之前的过程重新来过，而且祈祷数据上不会再出现问题。另外，几个月前的你对于现在来说就如同陌生人，你不可能记住六个月之前做过的与项目相关的具体工作，那么重新调试函数和数据将会成为噩梦。从另一方面，一个人的研究领域是很固定的，这就意味着一个人在相关领域的能力或只是储备是依靠不断积累来增长的，这就类似于盖房子，可重复性研究可以保证地基的稳定性，也就保证了研究的效率。

**提高准确性**：虽然可重复性的实现并不能保证结果的正确性，但它可以有效的提高结果的正确性，因为在实现可重复性的过程中，可以很容易的发现数据处理和分析过程中的错误

**提高可扩展性**：
数据科学领域往往有着大量重复或者类似的数据。这包括时间轴上的重复和结构层次上的重复。比如，网站的流量数据、股市的数据是时间层次上重复的的数据。在医学癌症领域，不同癌症之间的病理参数记录数据属于结构上重复的数据。分析这些重复或者类似的数据所采用的方法或者模型几乎相同。这意味着同一个科学研究成果可以被用于其他类似的数据上。而可重复性的研究成果可以通过简单的替换数据源来实现论文的创新。

**提高知识累计的速度和知识创新的速度**：
提供详细的计算过程使读者可以轻松的重复这些结果然后拓展，改进这种方法论。这可以有效的提高知识累计的效率

**可重复性的科研成果更有影响力**：
可重复性的研究成果中囊括了作者所有的工作，而不仅仅是论文中提供的简要版本。这使得读者可以根据自身的需要在不同的细节层次上重现作者的工作。


##九、可重复研究的改进的方向

本文只是提供了实现可重复性研究的一中可替代方案，该方案中可以有效提高可重复性程度。可重复性研究目前还有大量的问题需要解决，主要包括

**跨平台**：很自然的，我们希望在动态化报告中描述的任务可以通过不同的软件平台或者语言来实现。比如，有人可能使用Python来进行数据采集和清洗，使用命令行来组合文件，使用R语言进行统计分析和结果展示，使用javaScript建造可交互的展示。这些技术在上述可重复性的实现方案中已经有所体现，但目前这些不同技术的只能单独实现，而不能被整合到统一的平台下，目前，类似于Plotly的云平台已经在尝试将这些技术借助云技术统一起来，在plotly平台上，用户可以运行matlab，java，python，R等语言做数据分析，同时选取js，R等技术进行可视化。未来，云平台是实现可重复性研究的重要平台。
	
**大数据方案**：随着大数据时代的来临，数据的容量以指数的形式增长，数据容量的增长同时提高了对系统和平台性能的要求，同时也增加了可重复性的实现的难度。目前，两种主流的大数据解决方案，hadoop和spark都已经实现了对R的支持。这是未来可重复性研究的一个重要方向。
	
**交互性**：本文中提到的可重复性方案支持R shiny来实现可交互式方案。但是，如果想要实现更加个性化的需求，就需要使用web技术，比如CSS，html5以及Javascript来实现。但这就进一步增加了可重复性实现的难度。
	
对于未接触过本文中提到的可重复性实现的工具的研究人员而言，实现本文中的可重复性研究的方案需要不小的时间成本[^time]。可重复性研究与研究课题本身并不相干，一般的研究人员也不愿意花费额外的时间和精力维护代码来实现可重复性，这也是这一标准难以推广实施的重要原因。随着各大学术期刊对可重复性的标准不断的提升，这种状况会不断的好转。

[^time]: M. Schwab, M. Karrenbach, and J. Claerbout, Making scientific computations reproducible, Computing in Science & Engineering, vol. 2, no. 6, pp. 61–67, Nov. 2000. 
	
	



	
[^何华清]: 何华青,吴彤. 实验的可重复性研究——新实验主义与科学知识社会学比较[J]. 自然辩证法通讯,2008,04:42-48+111.
[^波蒂学术事件]: 王炳顺. 杜克大学波蒂事件及研究的可重复性[J]. 中国医学伦理学,2013,06:683-686.